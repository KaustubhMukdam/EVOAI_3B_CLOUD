model:
  name: "Qwen/Qwen2.5-Coder-3B-Instruct"
  device: "cuda"
  quantization: "4bit"
  max_length: 1024
  temperature: 0.7
  top_p: 0.9
  cache_dir: ".models/qwen_cache"

hardware:
  gpu_memory_fraction: 0.90
  gradient_checkpointing: true
  dataloader_num_workers: 2
  batch_size: 1
  gradient_accumulation_steps: 8

optimization:
  mixed_precision: "fp16"
  flash_attention: false  # Disabled for Windows
  memory_efficient_attention: true
  cpu_offload: true
  use_sdp_attention: true  # Use PyTorch's scaled dot product attention

constitutional:
  critique_enabled: true
  revision_enabled: true
  max_revisions: 2
